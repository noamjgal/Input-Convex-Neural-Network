# This is a first attempt at writing a program in PyTorch to simulate a ICNN. It is relatively simple, containing biases for all linear layers, using softplus activation for all, and no complicated structure.
import torch
from torch import nn, Tensor
import torch.nn.functional as F
import numpy as np
import torch.nn.init as init
import torch.optim as optim

class ICNN(torch.nn.Module):
	def __init__(self, input_size, hidden_layers_dim, num_hidden_layers, output_size):
		super(ICNN, self).__init__()
		self.input_size = dim_inputs
		self.hidden_layers_dim = hidden_layers_dim
		self.num_hidden_layers = num_hidden_layers
		self.output_size = output_size

		z_flow = list() # z_flow will become a list of linear layers. Represents the forward linear flow from y to z1 to z2 to ... zk
		z_flow.append(nn.Linear(input_size, hidden_layers_dim))
		for _ in range(num_hidden_layers - 1):
			z_flow.append(nn.Linear(hidden_layers_dim, hidden_layers_dim)
		z_flow.append(nn.Linear(hidden_layers_dim, output_size)
		self.z_traversal = torch.nn.ModuleList(z_flow)

		
		y_flow = list() # y_flow will become a list of linear layers. Represents the forward linear connections between y and z2, y and z3, ..., y and zk
		for _ in range(num_hidden_layers - 1):
			y_flow.append(nn.Linear(input_size, hidden_layers_dim)
		y_flow.append(nn.Linear(input_size, output_size)
		self.y_traversal = torch.nn.ModuleList(y_flow)

		self.activation_function = nn.Softplus() # Softplus is convex and nondecreasing thus satisfies Proposition 1's criteria for network convexity

	def forward(self, x):
		vector = self.activation(self.z_traversal[0](x)) # vector holds the activation of each neuron at a given step. Will end up as the final output vector of the neural network
		for z_flow, y_flow in zip(self.z_traversal[1:-1], self.y_traversal[:-1]): # I don't understand fully the python syntax here
			vector = self.activation_function(z_flow(vector) + y_flow(x)) # This is simply moving forward
		vector = self.z_traversal[-1](vector) + self.y_traversal[-1](x) # I believe -1 represents the last element in a list in python, that is the only thing which makes sense to me here
		return F.log_softmax(vector, dim=1) # This may be wrong

# This next section will run the forward and backward passes of the network, this is a rough draft so may have a bug or two
# icnn = ICNN(input_size, hidden_layers_dim, num_hidden_layers, output_size) # Input these bad boys
icnn = ICNN(1, 8, 1, 1)
# Define the data batches 'trainset' here. Here is the architecture: 'trainset' will have batch size 16, 2048 batches, and then some dimensionality of each x-value with its corresponding norm(x)^2
x_values = np.random.rand([1, 16]) * 10 # More to be written here

EPOCHS = 3
optimizer = optim.Adam(net.parameters(), lr=0.01)
for epoch in range(EPOCHS):
	for data in trainset:
		X, y = data
		output = icnn.forward(X)
		loss = F.nll_loss(output, y) # Will need to change this loss function to be one which works for regression
		loss.backward()
		optimize.step()
		for z_flow in z_traversal:
			z_flow.weight = max(0, z_flow.weight) # This should guarantee the z-flow weights stay non-negative



# The next class, PICNN, will be much more complex. It must account for the added architectual complexity of PICNN's, and more challenging implementation of PyTorch's pre-defined functions. Additionlly, the added complexity of the model makes optimization and appropriate alteration of the model much more challenging.
class PICNN(torch.nn.Module):
