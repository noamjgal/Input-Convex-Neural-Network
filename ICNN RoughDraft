'''Rough Draft of an Input Convex Neural Network (ICNN), built using PyTorch, that has a simple structure, 
contains biases for all linear layers, and uses a softplus activation function.'''

import torch
from torch import nn, Tensor
import torch.nn.functional as F
import torch.nn.init as init
import torch.optim as optim
import numpy as np

class ICNN(nn.Module):

	def __init__(self, input_size, hidden_layers_dim, num_hidden_layers, output_size):
		super(ICNN, self).__init__()
		self.input_size = input_size
		self.hidden_layers_dim = hidden_layers_dim
		self.num_hidden_layers = num_hidden_layers
		self.output_size = output_size

		# z_flow is a list of linear layers representing the forward linear flow from y to z1 to z2 to ... zk
		z_flow = list() 
		# nn.Linear applies a linear transformation to incoming data: output = input*weight + bias
		z_flow.append(nn.Linear(input_size, hidden_layers_dim))

		 # creating all weights in layer and changing negative ones to 0
		for i in range(num_hidden_layers - 1):
		    layer = nn.Linear(hidden_layers_dim, hidden_layers_dim)
		    # ensures that the z-flow weights are non-negative, which is a constraint from Proposition 1 of the ICNN paper
		    with torch.no_grad(): # idk what this is but it prevents error
			# search every weight for every neuron in layer
			for i in range(hidden_layers_dim): # loop through each neuron
			    for j in range(hidden_layers_dim): # loop through each weight
				layer.weight[i, j] = max(0, layer.weight[i, j])
			# print("Layer Weights: " + str(layer.weight))
		    z_flow.append(layer)

		# zk
		last_layer = nn.Linear(hidden_layers_dim, output_size)
		# searching all weights in layer and changing negative ones to 0
		with torch.no_grad(): # idk what this is but it prevents error
		    # search every weight for every neuron in layer. But output is generally dim 1 anyway
		    for i in range(output_size):
			for j in range(hidden_layers_dim): # loop through each weight
			    last_layer.weight[i, j] = max(0, last_layer.weight[i, j])
		    # print("Last Layer Weights: " + str(last_layer.weight))
		z_flow.append(last_layer)
		
		# nn.ModuleList outputs a list of all the modules in the inputted flow
		self.z_traversal = nn.ModuleList(z_flow)

		# y_flow is a list of linear layers representing the forward linear connections between y and z2, y and z3, ..., y and zk
		y_flow = list() 
		for layer in range(num_hidden_layers - 1):
			y_flow.append(nn.Linear(input_size, hidden_layers_dim))
		y_flow.append(nn.Linear(input_size, output_size))
		self.y_traversal = nn.ModuleList(y_flow)
		# Softplus is convex and nondecreasing thus satisfies Proposition 1's criteria for network convexity
		self.activation_function = nn.Softplus() 

	def forward(self, x):
		# vector holds the activation of each neuron at a given step and serves as the final output vector of the neural network
		vector = self.activation_function(self.z_traversal[0](x)) 
		for z_flow, y_flow in zip(self.z_traversal[1:-1], self.y_traversal[:-1]): 
			vector = self.activation_function(z_flow(vector) + y_flow(x)) 
		vector = self.z_traversal[-1](vector) + self.y_traversal[-1](x)
		return self.activation_function(vector) 
		

'''This next section will run test forward and backward passes of the network
icnn = ICNN(input_size, hidden_layers_dim, num_hidden_layers, output_size) Input these bad boys'''

icnn = ICNN(1, 8, 1, 1)

# Define the data batches 'trainset' here. 'trainset' architecture: batch size 16, 2048 batches, and then some dimensionality of each x-value with its corresponding norm(x)^2
num_of_batches = 2048
batch_size = 16
trainset = list()

# not sure whether this commented out code is still necessary
#batch = list()
#for _ in range(num_of_batches):
	#batch.append(np.random.rand(batch_size) * 2 - 1)
	#batch.append(batch[0] ** 2)
	#trainset.append(batch)
	#batch.clear()

# The following section generates the data in the form necessary to pass into the training loop
batch = np.zeros([batch_size,2])
for _ in range(num_of_batches): 
	batch = np.zeros([batch_size,2])
	for i in range(batch_size):
		batch[i,0] = np.random.rand() * 2 - 1
		batch[i,1] = batch[i,0] ** 2
	trainset.append(batch)

mse_loss = nn.MSELoss() # Mean-Squared Error Loss function is preferred for regression problems
EPOCHS = 3 # Number of times to loop through the data
optimizer = optim.Adam(icnn.parameters(), lr=0.001) # Adam optimizer is the best for our purposes
for epoch in range(EPOCHS):
	for data in trainset:
		X = data[:,0]
		y = data[:,1]
		output = icnn.forward(X) # Gotta fix the batch input rather than a single input
		loss = mse_loss(output, y)
		loss.backward()
		optimizer.step()
		for z_flow in icnn.z_traversal:
			z_flow.weight = max(0, z_flow.weight) # guarantees that the z-flow weights are non-negative



'''The next class, PICNN, will be much more complex. It must account for the added architectual complexity of PICNN's, 
and more challenging implementation of PyTorch's pre-defined functions. 
Additionlly, the added complexity of the model makes optimization and appropriate alteration of the model much more challenging.'''

class PICNN(nn.Module):
